{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhananjayd99/Python-/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load data\n",
        "train = pd.read_csv(\"/content/train.csv\", on_bad_lines=\"skip\", low_memory=False)\n",
        "test = pd.read_csv(\"/content/test.csv\")\n",
        "\n",
        "# Keep test ID\n",
        "test_ids = test[\"ID\"]\n",
        "\n",
        "# Extract target and drop ID and target from train\n",
        "y = train[\"Default 12 Flag\"]\n",
        "train.drop([\"ID\", \"Default 12 Flag\"], axis=1, inplace=True, errors=\"ignore\")\n",
        "test.drop([\"ID\"], axis=1, inplace=True, errors=\"ignore\")\n",
        "\n",
        "# Drop non-numeric columns (you can extract date features later if needed)\n",
        "combined = pd.concat([train, test], axis=0)\n",
        "for col in combined.select_dtypes(include=\"object\").columns:\n",
        "    combined[col] = combined[col].astype(str)\n",
        "    le = LabelEncoder()\n",
        "    combined[col] = le.fit_transform(combined[col])\n",
        "\n",
        "# Impute missing values\n",
        "imputer = SimpleImputer(strategy=\"median\")\n",
        "combined = pd.DataFrame(imputer.fit_transform(combined), columns=combined.columns)\n",
        "\n",
        "# Split back\n",
        "X = combined.iloc[:len(train)]\n",
        "X_test = combined.iloc[len(train):]\n",
        "\n",
        "# Cross-validation\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_preds = np.zeros(len(X))\n",
        "test_preds = np.zeros(len(X_test))\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "    X_tr, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
        "    y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
        "\n",
        "    model = GradientBoostingClassifier(\n",
        "        n_estimators=200,\n",
        "        learning_rate=0.03,\n",
        "        max_depth=4,\n",
        "        min_samples_leaf=20,\n",
        "        random_state=42\n",
        "    )\n",
        "    model.fit(X_tr, y_tr)\n",
        "\n",
        "    oof_preds[val_idx] = model.predict_proba(X_val)[:, 1]\n",
        "    test_preds += model.predict_proba(X_test)[:, 1] / kf.n_splits\n",
        "\n",
        "    fold_auc = roc_auc_score(y_val, model.predict_proba(X_val)[:, 1])\n",
        "    print(f\"ðŸ“ˆ Fold {fold+1} AUC: {fold_auc:.4f}\")\n",
        "\n",
        "# Final AUC\n",
        "print(f\"\\nâœ… Full Cross-Validation AUC: {roc_auc_score(y, oof_preds):.4f}\")\n",
        "\n",
        "# Save submission\n",
        "submission = pd.DataFrame({\n",
        "    \"ID\": test_ids,\n",
        "    \"Default 12 Flag\": test_preds\n",
        "})\n",
        "submission.to_csv(\"submission_cv_gb.csv\", index=False)\n",
        "print(\"âœ… submission_cv_gb.csv saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "7e_hbzuB0NWy",
        "outputId": "94464af3-0f47-45c2-f6e9-9a88011cb3da"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1656129104.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"skip\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/train.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import roc_auc_score, make_scorer\n",
        "from scipy.stats import uniform, randint\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# --- 1. Data Loading ---\n",
        "\n",
        "# Load the datasets\n",
        "try:\n",
        "    train_df = pd.read_csv('train.csv')\n",
        "    test_df = pd.read_csv('test.csv')\n",
        "    sample_submission = pd.read_csv('sample_submission.csv')\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: One of the files was not found. Please ensure all files are available. {e}\")\n",
        "    exit()\n",
        "\n",
        "print(\"âœ… Data loaded successfully.\")\n",
        "\n",
        "# --- 2. Preprocessing and Feature Engineering Function ---\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Performs feature engineering and converts types.\"\"\"\n",
        "\n",
        "    df['Application Date'] = pd.to_datetime(df['Application Date'])\n",
        "    df['Date of Birth'] = pd.to_datetime(df['Date of Birth'], errors='coerce')\n",
        "\n",
        "    # Feature Engineering: Age and Days since application\n",
        "    current_year = 2019\n",
        "    df['Age'] = current_year - df['Date of Birth'].dt.year\n",
        "    df['Days_Since_Application'] = (datetime(2020, 1, 1) - df['Application Date']).dt.days\n",
        "\n",
        "    # Drop original date/time columns\n",
        "    df = df.drop(columns=['Application Date', 'Application Time', 'Date of Birth'], errors='ignore')\n",
        "\n",
        "    # Store ID and drop the column from features\n",
        "    df_id = df.pop('ID') if 'ID' in df.columns else None\n",
        "\n",
        "    # Handle 'JIS Address Code'\n",
        "    if 'JIS Address Code' in df.columns:\n",
        "        df['JIS Address Code'] = df['JIS Address Code'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
        "\n",
        "    # Log Transform Skewed Numerical Features\n",
        "    for col in ['Total Annual Income', 'Amount of Unsecured Loans',\n",
        "                'Declared Amount of Unsecured Loans', 'Rent Burden Amount']:\n",
        "        if col in df.columns:\n",
        "            df[col] = np.log1p(df[col])\n",
        "\n",
        "    # Convert all object/string columns to 'category' dtype\n",
        "    for col in df.select_dtypes(include=['object']).columns:\n",
        "        df[col] = df[col].astype('category')\n",
        "\n",
        "    return df, df_id\n",
        "\n",
        "# Apply preprocessing\n",
        "train_df_processed, _ = preprocess_data(train_df.copy())\n",
        "test_df_processed, test_id = preprocess_data(test_df.copy())\n",
        "\n",
        "# --- 3. Align and Prepare Data for Modeling (HGB) ---\n",
        "\n",
        "TARGET = 'Default 12 Flag'\n",
        "y = train_df_processed[TARGET]\n",
        "X = train_df_processed.drop(columns=[TARGET])\n",
        "\n",
        "# Combine for alignment\n",
        "full_data = pd.concat([X, test_df_processed], keys=['train', 'test'])\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = [col for col in full_data.columns if full_data[col].dtype.name == 'category']\n",
        "\n",
        "# CRITICAL FIX: Frequency Encoding for High-Cardinality features\n",
        "HIGH_CARDINALITY_THRESHOLD = 255\n",
        "low_card_cols = []\n",
        "high_card_cols = []\n",
        "\n",
        "for col in categorical_cols:\n",
        "    if len(full_data.loc['train', col].cat.categories) > HIGH_CARDINALITY_THRESHOLD:\n",
        "        high_card_cols.append(col)\n",
        "\n",
        "        # Frequency Encoding\n",
        "        freq_map = full_data[col].value_counts(normalize=True).to_dict()\n",
        "        full_data[f'{col}_Freq'] = full_data[col].map(freq_map)\n",
        "\n",
        "        # Drop the original high-cardinality column\n",
        "        full_data = full_data.drop(columns=[col])\n",
        "    else:\n",
        "        low_card_cols.append(col)\n",
        "        # For low-cardinality, impute 'Missing' as a category\n",
        "        full_data[col] = full_data[col].cat.add_categories('Missing').fillna('Missing')\n",
        "        full_data[col] = full_data[col].astype('category')\n",
        "\n",
        "\n",
        "# Impute remaining numerical NaNs\n",
        "numerical_cols = full_data.select_dtypes(include=np.number).columns\n",
        "for col in numerical_cols:\n",
        "    full_data[col] = full_data[col].fillna(full_data.loc['train', col].median())\n",
        "\n",
        "# Separate back into training and testing sets\n",
        "X_aligned = full_data.loc['train']\n",
        "test_aligned = full_data.loc['test']\n",
        "\n",
        "# --- 4. Model Training and Tuning (HistGradientBoostingClassifier) ---\n",
        "\n",
        "# Split data into 80% train / 20% validation for tuning and final evaluation\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_aligned, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Identify the indices of the remaining low-cardinality categorical features\n",
        "categorical_features_indices_final = [\n",
        "    i for i, col in enumerate(X_train.columns)\n",
        "    if X_train[col].dtype.name == 'category'\n",
        "]\n",
        "\n",
        "# Define the model base\n",
        "hgb_base = HistGradientBoostingClassifier(\n",
        "    max_iter=1500,\n",
        "    early_stopping=True,\n",
        "    n_iter_no_change=150,\n",
        "    tol=1e-7,\n",
        "    random_state=42,\n",
        "    categorical_features=categorical_features_indices_final\n",
        ")\n",
        "\n",
        "# Define the parameter search space\n",
        "param_dist = {\n",
        "    'learning_rate': uniform(loc=0.01, scale=0.10), # Search between 0.01 and 0.11\n",
        "    'max_depth': randint(low=5, high=15),          # Search depths between 5 and 14\n",
        "    'max_leaf_nodes': randint(low=10, high=50),     # Search leaf nodes between 10 and 49\n",
        "    'l2_regularization': uniform(loc=0.1, scale=1.0) # Search between 0.1 and 1.1\n",
        "}\n",
        "\n",
        "# Use AUC as the scoring metric\n",
        "auc_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=hgb_base,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=20, # Number of different parameter combinations to try\n",
        "    scoring=auc_scorer,\n",
        "    cv=3, # 3-fold cross-validation\n",
        "    random_state=42,\n",
        "    verbose=0,\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "# Train the tuner (using X_train/y_train for CV folds)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_hgb = random_search.best_estimator_\n",
        "\n",
        "# Evaluate the best model on the held-out validation set\n",
        "val_probabilities = best_hgb.predict_proba(X_val)[:, 1]\n",
        "val_auc = roc_auc_score(y_val, val_probabilities)\n",
        "\n",
        "# --- 5. Prediction and Submission ---\n",
        "\n",
        "# Predict probabilities on the test set\n",
        "test_probabilities = best_hgb.predict_proba(test_aligned)[:, 1]\n",
        "\n",
        "# Create submission file\n",
        "submission_df = pd.DataFrame({'ID': test_id, 'Default 12 Flag': test_probabilities})\n",
        "\n",
        "# Save to CSV\n",
        "submission_df.to_csv('submission_hgb_tuned.csv', index=False)\n",
        "\n",
        "print(\"\\n--- Execution Summary (Tuned HGB Model) ---\")\n",
        "print(f\"Model: HistGradientBoostingClassifier (Tuned)\")\n",
        "print(f\"Best Hyperparameters: {random_search.best_params_}\")\n",
        "print(f\"AUC on Validation Set: {val_auc:.4f}\")\n",
        "print(\"Submission file 'submission_hgb_tuned.csv' has been generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "EbD0nRvx2J_g",
        "outputId": "218e2400-7741-4200-fd21-0f86e8ae3bf8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 31 fields in line 41493, saw 54\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2020563748.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Load the datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0msample_submission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sample_submission.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 31 fields in line 41493, saw 54\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l01c01_introduction_to_colab_and_python.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}